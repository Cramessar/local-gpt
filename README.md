# ðŸ’» Local GPT

Local-first AI assistant with full document RAG, offline LLM chat, system monitoring, and Dockerized deployment.

---

## âœ… Whatâ€™s Working

| Feature | Status | Notes |
|---------|--------|--------|
| âœ… Local Chat (vLLM / Ollama) | Fully working | Supports openai-compatible models via vLLM & Ollama models locally |
| âœ… Docs Upload + RAG | Fully working | PDF/DOCX/TXT uploaded, chunked, stored in ChromaDB, and queried in chat |
| âœ… Toolserver API | Fully working | `/rag/upload`, `/rag/list`, `/tool (rag_query)`, etc. |
| âœ… Resource Monitoring | Fully working | GPU/CPU/RAM shown in UI using psutil + NVML |
| âœ… Dockerized Setup | Fully working | One command brings up entire stack |
| âš™ï¸ PDF Text Extraction | Working | Uses `pdfplumber` & `PyMuPDF` |
| âš™ï¸ Embeddings | Working | ONNX MiniLM L6-V2 or SentenceTransformer fallback |
| âš™ï¸ ChromaDB Vector Store | Working | Persistent at `/data/chroma_v2` |
| ðŸ›  RAG in Chat | Working | Injects context as system messages when enabled |

---

## ðŸ“‚ Updated Project Structure
```
local-gpt/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ backend/ # FastAPI Toolserver
â”‚ â”œâ”€â”€ app.py # FastAPI entry
â”‚ â”œâ”€â”€ tool_router.py # /tool â†’ rag_query, rag_recent, etc.
â”‚ â”œâ”€â”€ rag_routes.py # /rag/upload, /rag/list
â”‚ â”œâ”€â”€ vectorstore.py # ChromaDB logic (add/query)
â”‚ â”œâ”€â”€ file_extract.py # PDF/DOCX parsing
â”‚ â”œâ”€â”€ tools/
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ rag.py # rag_upsert implementation
â”‚ â”‚ â”œâ”€â”€ system.py # System tool example
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ web/ (or frontend/) # Next.js UI
â”‚ â”œâ”€â”€ next.config.js
â”‚ â”œâ”€â”€ package.json
â”‚ â”œâ”€â”€ pages/
â”‚ â”‚ â”œâ”€â”€ index.tsx # Main chat UI + RAG sidebar
â”‚ â”‚ â”œâ”€â”€ api/chat.ts # Handles RAG + chat -> vLLM/Ollama
â”‚ â”œâ”€â”€ components/
â”‚ â”‚ â”œâ”€â”€ ChatWindow.tsx
â”‚ â”‚ â”œâ”€â”€ DocsRagPanel.tsx # Upload + index docs UI
â”‚ â”‚ â”œâ”€â”€ ResourcePanel.tsx # CPU/GPU monitor
â”‚ â””â”€â”€ lib/
â”‚ â”œâ”€â”€ llm.ts # callOpenAI / callOllama client
â”‚ â”œâ”€â”€ toolserver.ts # helper for hitting /tool
â”‚
â””â”€â”€ data/ (Docker volume)
â”œâ”€â”€ files/ # Uploaded docs
â””â”€â”€ chroma_v2/ # Vector DB
```


---

## ðŸš€ Run with Docker

```bash
docker compose up --build
```
Frontend â†’ http://localhost:3000

Toolserver â†’ http://localhost:8000

Uploaded files â†’ saved in Docker volume /data/files

---

| Step | What Happens |
|------|---------------|
| 1ï¸âƒ£ Upload | PDF / DOCX / TXT in sidebar |
| 2ï¸âƒ£ Process | Toolserver extracts â†’ chunks â†’ embeds â†’ stores in ChromaDB |
| 3ï¸âƒ£ Enable RAG | Toggle **Use docs (RAG)** in the chat UI |
| 4ï¸âƒ£ Query | API calls `POST /tool (rag_query)` and retrieves top-k chunks |
| 5ï¸âƒ£ LLM Answer | Chunks are injected as context â†’ local LLM answers using your content |


---
ðŸ§  Example RAG Prompt
After uploading Software_Engineer.pdf:

```pgsql

Using Software_Engineer.pdf in collection default,
summarize the Summary section and suggest improvements.
```
Or a tool-style prompt:
```
"Use the docs I uploaded to answer:
What programming languages are mentioned in my resume?"
```
---

```mermaid
sequenceDiagram
  participant User
  participant UI
  participant API
  participant VS as ChromaDB
  participant LLM

  User->>UI: Ask question (Use docs ON)
  UI->>API: POST /tool { name: rag_query }
  API->>VS: Query top-k embeddings
  VS-->>API: Return chunks
  API->>LLM: Chat completion with context
  LLM-->>API: Answer
  API-->>UI: Response

```

---

âœ… Next Possible Improvements
 Add authentication (multi-user)

 Add markdown & PowerPoint extraction

 GUI for browsing indexed chunks

 Switch to LiteLLM or OpenAI function calling

 Save chat history per user

---

ðŸ“„ License
MIT â€” free to modify, share, and build on.

Made with â˜• + ðŸ¤– by Christopher Ramessar
Because boredom needed a hobby.