services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports: ["11434:11434"]
    volumes:
      - ollama_models:/root/.ollama
    # Use GPU if available (Docker Desktop + WSL2 + NVIDIA)
    gpus: all   # remove this line if CPU-only

  toolserver:
    build: ./backend
    container_name: toolserver
    ports: ["8000:8000"]
    environment:
      - RAG_DB_PATH=/data/chroma_v2
      - CHROMA_ANONYMIZED_TELEMETRY=false
      - FILE_SANDBOX=/data/files
      - ENABLE_GPU_METRICS=true
      - VLLM_HOST=vllm
      - VLLM_PORT=8000 
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ./backend:/app
      - toolserver_data:/data
    depends_on: [ollama]
    gpus: all

  web:
    build: ./web
    container_name: localgpt-web
    ports: ["5173:5173"]
    environment:
      - OLLAMA_URL=http://ollama:11434
      - TOOLS_URL=http://toolserver:8000
      - OPENAI_BASE_URL=http://vllm:8000/v1   # for vLLM (OpenAI-compatible)
      - OPENAI_API_KEY=placeholder            # vLLM ignores value but header is required
      - WATCHPACK_POLLING=true                # hot reload on Windows/WSL2
      - CHOKIDAR_USEPOLLING=true
      
    volumes:
      - ./web:/usr/src/app
      - /usr/src/app/node_modules
    depends_on: [ollama, toolserver, vllm]

  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    gpus: all                  # remove if CPU-only (but 20B really wants a GPU)
    ports:
      - "8001:8000"            # host:container
    volumes:
      - vllm_models:/models    # cache downloaded weights
    # If HF downloads need auth, add:
    # environment:
    #   - HF_TOKEN=${HF_TOKEN}
    command: >
      --model openai/gpt-oss-20b
      --download-dir /models
      --dtype bfloat16
      --max-model-len 8192
      --tensor-parallel-size 1
      --trust-remote-code

volumes:
  ollama_models:
  toolserver_data:
  vllm_models:
